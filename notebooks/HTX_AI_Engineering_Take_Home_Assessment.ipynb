{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTX AI Engineering Take-Home Assessment\n",
    "\n",
    "Done by Lee Zhan Peng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyMuPDF==1.26.5\n",
    "!pip install python-multipart==0.0.20\n",
    "!pip install langchain-core==1.0.2\n",
    "!pip install langchain_openai==1.0.1\n",
    "!pip install langchain-community==0.4.1\n",
    "!pip install mcp==1.20.0\n",
    "!pip install python-dateutil==2.9.0.post0\n",
    "!pip install langgraph==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dateutil import parser\n",
    "from pydantic import create_model, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import ToolMessage, SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_community.document_loaders.parsers.pdf import PyMuPDFParser\n",
    "from langchain_core.document_loaders import Blob\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from mcp.types import Tool, TextContent\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"../documents/fy2024_analysis_of_revenue_and_expenditure.pdf\"\n",
    "\n",
    "LLM_API_KEY=\"<REDACTED>\"\n",
    "LLM_BASE_URL=\"<REDACTED>\"\n",
    "LLM_MODEL_NAME=\"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Document Extraction & Prompt Engineering with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Reading Functions\n",
    "\n",
    "For reading of PDF, my initial thought was to have it read with any text extractor like pypdf. With each page, I classify, maybe with an LLM, on what structure exists in that page, e.g. table, graph, texts etc.\n",
    "\n",
    "With that classification, we then reread the PDF on that particular page with tools that are best for the various structures. E.g. Table: Camelot, Texts: PyMuPDF.\n",
    "\n",
    "However, looking at the given PDF source, the structure of the context, including texts and tables, are generally very well organised. To reduce research overhead, I decided to pick one reader that can achieve the fastest and most accurate reading, and that decision was `PyMuPDF`.\n",
    "\n",
    "- https://github.com/py-pdf/benchmarks suggests that PyMuPDF is fastest, and performance is good.\n",
    "- https://arxiv.org/pdf/2410.09871 paper suggests PyMuPDF is also well-performing, so it was chosen without much hesitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api/app.py: L43-54\n",
    "def read_pdf(file_path):\n",
    "    pdf_texts = {}\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        pdf_bytes = f.read()\n",
    "    \n",
    "    blob = Blob.from_data(pdf_bytes, mime_type=\"application/pdf\")\n",
    "    \n",
    "    parser = PyMuPDFParser()\n",
    "    documents = parser.parse(blob)\n",
    "    \n",
    "    for i, doc in enumerate(documents, start=1):\n",
    "        pdf_texts[i] = doc.page_content\n",
    "    \n",
    "    return pdf_texts\n",
    "\n",
    "# api/app.py: L87-88\n",
    "def get_pages_text(pdf_texts, pages):\n",
    "    texts = []\n",
    "    \n",
    "    for page_num in pages:\n",
    "        if page_num in pdf_texts:\n",
    "            texts.append(f\"\\n--- Page {page_num} ---\\n{pdf_texts[page_num]}\")\n",
    "    \n",
    "    return \"\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "The general strategy involves providing examples, almost like an N-shot prompting strategy, to give the LLM a guide on how the extraction of fields should happen.\n",
    "\n",
    "I wanted to opt for chain-of-thought, but it does not seem appropriate for data extraction, hence I decide to not have it in, but instead enforce reasoning via LLM structured output later.\n",
    "\n",
    "I have also included the part about the `date_normaliser` tool, in anticipation for Part 2 of the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\prompts\\data_extraction.py\n",
    "DATA_EXTRACTION_SYSTEM_MESSAGE=\"\"\"You are a data extraction assistant. Your task is to extract specific information from documents with high accuracy.\n",
    "\n",
    "Guidelines:\n",
    "- Extract ONLY the requested information, nothing more\n",
    "- Return the minimal exact value without surrounding context or explanatory text\n",
    "- Be precise and literal with numbers, values and quotes. Meaning if the units exist, include them.\n",
    "- If the requested information is not found, respond with null\n",
    "- For lists, extract all items that match the criteria\n",
    "- Do not infer or calculate values not explicitly stated in the document\n",
    "- Do not include explanatory phrases like \"does not apply to\" or \"refers to\" - extract only the specific data point requested\n",
    "\n",
    "Examples:\n",
    "- If asked for \"the date related to Maxwell's concert\" from \"Maxwell wanted to host his concert on 28 July 2018\", extract only \"28 July 2018\"\n",
    "- If asked for \"the price\" from \"The total cost is $500\", extract only \"$500\"\n",
    "- If a list of items is requested, then sure, extract all relevant items exactly as they appear\n",
    "\n",
    "CRITICAL - Tool Usage for Dates:\n",
    "- You have access to a normalise_date tool that MUST be used for ANY date extraction\n",
    "- If you extract ANY date (day, month, year, or any combination), you are REQUIRED to call the normalise_date tool\n",
    "- Examples of dates that require the tool: \"3 January 2028\", \"11 December 2009\", etc.\n",
    "- The tool will format the date correctly - you cannot skip this step\n",
    "- After calling the tool, use the normalised result as your value\n",
    "- Failure to use the normalise_date tool for date extractions is incorrect\"\"\"\n",
    "\n",
    "DATA_EXTRACTION_USER_MESSAGE=\"\"\"Extract the exact string information from the provided text:\n",
    "\n",
    "Request: {request}\n",
    "\n",
    "Document text:\n",
    "{text}\n",
    "\n",
    "MANDATORY: If the extracted information contains a date in ANY format, you MUST call the normalise_date tool before providing your final answer. This is not optional - all dates must be normalised using the tool.\"\"\"\n",
    "\n",
    "DATA_EXTRACTION_FINAL_INSTRUCTION=\"\"\"Provide your final answer with: original_text (the exact extracted text), value (formatted extracted text), and reason.\n",
    "Your formatted value must have the type of: {output_type}\n",
    "If suppose theres units involve, say 20 million, the final formatted value should be 20000000 if int is given or 20000000.0 if float. This is important, so if the extracted text is unclear, refer back to the full text.\n",
    "\n",
    "Also, you should refer back to the full text if the extracted text has made the value unclear as to which is correct.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction Chain\n",
    "\n",
    "A custom chain that assists with data extraction.\n",
    "In the extraction function, it will take in the requested field, the output data type, and the text from the PDF.\n",
    "\n",
    "We allow for the intake of tools for Part 2 of the assessment.\n",
    "\n",
    "In my original code, the chain actually takes in an MCP server. However, since we are running on Jupyter notebook, it's not ideal to run local MCP, so it is swapped to take in tools directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\chains\\data_extraction.py\n",
    "class DataExtractionChain:\n",
    "    def __init__(self, model, tools=None):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=LLM_API_KEY,\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        if tools:\n",
    "            # Basically the schema of the tools\n",
    "            self.tool_definitions = [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool[\"definition\"].name,\n",
    "                        \"description\": tool[\"definition\"].description,\n",
    "                        \"parameters\": tool[\"definition\"].inputSchema\n",
    "                    }\n",
    "                } for tool in tools\n",
    "            ]\n",
    "\n",
    "            # The functions to run for the tools\n",
    "            self.tool_executables = {tool[\"definition\"].name: tool[\"executable\"] for tool in tools}\n",
    "\n",
    "    def extract(self, request, output_type, text):\n",
    "        # Build initial messages\n",
    "        user_message = DATA_EXTRACTION_USER_MESSAGE.format(\n",
    "            request=request,\n",
    "            text=text\n",
    "        ) + \"\\n\\n\" + DATA_EXTRACTION_FINAL_INSTRUCTION.format(output_type=output_type)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=DATA_EXTRACTION_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=user_message)\n",
    "        ]\n",
    "\n",
    "        return self._extract_with_structure(messages, output_type)\n",
    "    \n",
    "    # For part 2 of the assessment\n",
    "    def extract_with_tools(self, request, output_type, text):\n",
    "        llm_with_tools = self.llm.bind(tools=self.tool_definitions)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=DATA_EXTRACTION_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=DATA_EXTRACTION_USER_MESSAGE.format(\n",
    "                request=request,\n",
    "                text=text\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        # Setting a max iteration so that if the LLM decides to go crazy and loop the tool call many times, it will be capped.\n",
    "        max_iterations = 10\n",
    "        for _ in range(max_iterations):\n",
    "            response = llm_with_tools.invoke(messages)\n",
    "\n",
    "            if not response.tool_calls:\n",
    "                # No tool calls\n",
    "                messages.append(AIMessage(content=response.content))\n",
    "                messages.append(HumanMessage(content=DATA_EXTRACTION_FINAL_INSTRUCTION.format(output_type=output_type)))\n",
    "                return self._extract_with_structure(messages, output_type)\n",
    "\n",
    "            messages.append(response)\n",
    "\n",
    "            for tool_call in response.tool_calls:\n",
    "                # Printing here so that we can see it when we run part 2.\n",
    "                print(f'Tool calling executed! Running {tool_call[\"name\"]}...')\n",
    "\n",
    "                tool = self.tool_executables[tool_call[\"name\"]]\n",
    "                tool_result = tool(tool_call[\"args\"])\n",
    "\n",
    "                tool_message = ToolMessage(\n",
    "                    content=str(tool_result),\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                )\n",
    "                messages.append(tool_message)\n",
    "\n",
    "    def _extract_with_structure(self, messages, output_type):\n",
    "        format_class = self._get_format_class(output_type)\n",
    "        llm_with_structure = self.llm.with_structured_output(format_class)\n",
    "        return llm_with_structure.invoke(messages)\n",
    "\n",
    "    def _get_format_class(self, output_type):\n",
    "        type_map = {\n",
    "            \"str\": str,\n",
    "            \"int\": int,\n",
    "            \"float\": float,\n",
    "            \"list[str]\": list[str],\n",
    "            \"list[int]\": list[int],\n",
    "            \"list[float]\": list[float],\n",
    "        }\n",
    "        \n",
    "        value_type = type_map[output_type]\n",
    "        \n",
    "        return create_model(\n",
    "            \"Format\",\n",
    "            original_text=(str, ...),\n",
    "            value=(value_type | None, None),\n",
    "            reason=(str, ...),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fields to extract\n",
    "\n",
    "We create a list of fields to iterate through later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from assessment PDF\n",
    "fields_to_extract = [\n",
    "    {\n",
    "        \"pages\": [5],\n",
    "        \"description\": \"Amount of Corporate Income Tax in 2023\",\n",
    "        \"output_type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"pages\": [5],\n",
    "        \"description\": \"YOY percentage difference of Corp Income Tax in 2023\",\n",
    "        \"output_type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"pages\": [20],\n",
    "        \"description\": \"Total amount of top ups in 2024\",\n",
    "        \"output_type\": \"float\"\n",
    "    },\n",
    "    {\n",
    "        \"pages\": [5, 6],\n",
    "        \"description\": \"List of taxes mentioned in section 'Operating Revenue'\",\n",
    "        \"output_type\": \"list[str]\"\n",
    "    },\n",
    "    {\n",
    "        \"pages\": [8],\n",
    "        \"description\": \"Latest Actual Fiscal Position in billions\",\n",
    "        \"output_type\": \"float\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Execution\n",
    "\n",
    "We read the PDF, iterate through the fields to extract, and pass them into our DataExtractionChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \"Amount of Corporate Income Tax in 2023\", Pages: 5, Expected data type: <float>\n",
      "EXTRACTED: 28400000000.0\n",
      "DATA TYPE: <class 'float'>\n",
      "================================================================================\n",
      "Extracting \"YOY percentage difference of Corp Income Tax in 2023\", Pages: 5, Expected data type: <float>\n",
      "EXTRACTED: 17.0\n",
      "DATA TYPE: <class 'float'>\n",
      "================================================================================\n",
      "Extracting \"Total amount of top ups in 2024\", Pages: 20, Expected data type: <float>\n",
      "EXTRACTED: 20352000000.0\n",
      "DATA TYPE: <class 'float'>\n",
      "================================================================================\n",
      "Extracting \"List of taxes mentioned in section 'Operating Revenue'\", Pages: 5, 6, Expected data type: <list[str]>\n",
      "EXTRACTED: ['Corporate Income Tax', 'Other Taxes', 'Vehicle Quota Premiums', 'Personal Income Tax', 'Assets Taxes', 'Betting Taxes']\n",
      "DATA TYPE: <class 'list'>\n",
      "================================================================================\n",
      "Extracting \"Latest Actual Fiscal Position in billions\", Pages: 8, Expected data type: <float>\n",
      "EXTRACTED: 1720000000.0\n",
      "DATA TYPE: <class 'float'>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "pdf_texts = read_pdf(PDF_PATH)\n",
    "\n",
    "extraction_chain = DataExtractionChain(model=LLM_MODEL_NAME)\n",
    "\n",
    "for fields in fields_to_extract:\n",
    "    pages, description, output_type = fields[\"pages\"], fields[\"description\"], fields[\"output_type\"]\n",
    "\n",
    "    print(f'Extracting \\\"{description}\\\", Pages: {\", \".join(map(str, pages))}, Expected data type: <{output_type}>')\n",
    "\n",
    "    # Combine extracted text if multiple pages are involved\n",
    "    stringified_pdf_text = get_pages_text(pdf_texts, pages)\n",
    "\n",
    "    # Because we use structured output in our chain, the output will look like this:\n",
    "    # {\n",
    "    #     original_text: str\n",
    "    #     value: output_type\n",
    "    #     reason: str\n",
    "    # }\n",
    "    output = extraction_chain.extract(\n",
    "        description,\n",
    "        output_type,\n",
    "        stringified_pdf_text\n",
    "    )\n",
    "    extracted_value = output.value\n",
    "\n",
    "    print(f\"EXTRACTED: {extracted_value}\")\n",
    "    print(f\"DATA TYPE: {type(extracted_value)}\")\n",
    "    print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tool Calling & Reasoning Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool: Date Normaliser\n",
    "\n",
    "It will be appropriate to actually use langchain's tool implementation for this, but in my codebase I implemented with python MCP library for the local MCP, and so I will stick to that.\n",
    "\n",
    "Have opted the use of tool directly into the LLM call here instead of curating a local MCP like my application. The logic stays consistent, except that we skip the MCP communication and directly use the tools where applicable.\n",
    "\n",
    "The tool calling for date normalisation is not a hard-coded process, and it's decided by the LLM.\n",
    "\n",
    "The tool definition is basically the schema, and the executable is the function that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\mcp_servers\\tools\\date_normaliser.py: L4-17\n",
    "date_normaliser_definition = Tool(\n",
    "    name=\"normalise_date\",\n",
    "    description=\"Normalise date strings like '21 March 2021' or '4 December 2002' to ISO format (YYYY-MM-DD)\",\n",
    "    inputSchema={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"date_string\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The date string to normalise (e.g., '21 March 2021')\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"date_string\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# api\\mcp_servers\\tools\\date_normaliser.py: L19-29\n",
    "def date_normaliser_executable(arguments):\n",
    "    date_string = arguments.get(\"date_string\", \"\")\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_string)\n",
    "        normalized = parsed_date.strftime(\"%Y-%m-%d\")\n",
    "        return [TextContent(type=\"text\", text=normalized)]\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: Could not parse date '{date_string}'. {str(e)}\"\n",
    "        return [TextContent(type=\"text\", text=error_msg)]\n",
    "    \n",
    "date_normaliser = {\"definition\": date_normaliser_definition, \"executable\": date_normaliser_executable}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date fields to extract\n",
    "\n",
    "We create a list of fields to iterate through later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from assessment PDF\n",
    "date_fields_to_extract = [\n",
    "    {\n",
    "        \"pages\": [1],\n",
    "        \"description\": \"The document's distribution date\",\n",
    "        \"output_type\": \"str\"\n",
    "    },\n",
    "    {\n",
    "        \"pages\": [36],\n",
    "        \"description\": \"The date relating to estate duty\",\n",
    "        \"output_type\": \"str\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Classifier Chain\n",
    "\n",
    "For this part of the assessment, we will additionally use an LLM to reason and classify the extracted date against `2024-01-01`.\n",
    "\n",
    "As our extraction does not seem to entail a date range, we assume the following:\n",
    "- Expired: The date is before `2024-01-01`\n",
    "- Upcoming: The date is after `2024-01-01`\n",
    "- Ongoing: The date is `2024-01-01`\n",
    "\n",
    "We will curate a set of prompt, plus another custom `DateClassifierChain` to help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\prompts\\date_classifier.py\n",
    "DATE_CLASSIFIER_SYSTEM_MESSAGE = \"\"\"You are a date classification assistant. Your task is to classify dates relative to a reference date.\n",
    "\n",
    "You must categorise each date into one of three states:\n",
    "- Expired: The date is before the reference date\n",
    "- Upcoming: The date is after the reference date\n",
    "- Ongoing: The date is the same as the reference date\n",
    "\n",
    "Guidelines:\n",
    "- Compare the normalised date against the reference date carefully\n",
    "- Be precise with date comparisons, considering day, month, and year\n",
    "- Use simple comparison: before = Expired, after = Upcoming, same = Ongoing\"\"\"\n",
    "\n",
    "DATE_CLASSIFIER_USER_MESSAGE = \"\"\"Classify the following normalised date against the reference date:\n",
    "\n",
    "Normalised Date: {normalised_date}\n",
    "Reference Date: {reference_date}\n",
    "\n",
    "Analyse the date, determine and give reason on whether it is Expired, Upcoming, or Ongoing relative to the reference date.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\chains\\date_classifier.py\n",
    "class DateClassifierChain:\n",
    "    def __init__(self, model):\n",
    "        # Use ChatOpenAI for OpenAI compatible models\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=LLM_API_KEY,\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    def classify(self, normalised_date, reference_date=\"2024-01-01\"):\n",
    "        messages = [\n",
    "            SystemMessage(content=DATE_CLASSIFIER_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=DATE_CLASSIFIER_USER_MESSAGE.format(\n",
    "                normalised_date=normalised_date,\n",
    "                reference_date=reference_date\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        format_class = self._get_format_class()\n",
    "        llm_with_structure = self.llm.with_structured_output(format_class)\n",
    "        return llm_with_structure.invoke(messages)\n",
    "\n",
    "    def _get_format_class(self):\n",
    "        return create_model(\n",
    "            \"DateClassification\",\n",
    "            normalised_date=(str, ...),\n",
    "            reference_date=(str, ...),\n",
    "            reason=(str, ...),\n",
    "            classification=(str, ...),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Execution\n",
    "\n",
    "Unlike part 1, we include the `date_normaliser` tool into the DataExtractionChain.\n",
    "\n",
    "We will also classify the dates after date extraction is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting \"The document's distribution date\", Pages: 1, Expected data type: <str>\n",
      "Tool calling executed! Running normalise_date...\n",
      "EXTRACTED: 2024-02-16\n",
      "----------\n",
      "Classifying the date...\n",
      "CLASSIFICATION: Upcoming\n",
      "REASONING: The normalised date 2024-02-16 is after the reference date 2024-01-01.\n",
      "================================================================================\n",
      "Extracting \"The date relating to estate duty\", Pages: 36, Expected data type: <str>\n",
      "Tool calling executed! Running normalise_date...\n",
      "EXTRACTED: 2008-02-15\n",
      "----------\n",
      "Classifying the date...\n",
      "CLASSIFICATION: Expired\n",
      "REASONING: The normalised date 2008-02-15 is before the reference date 2024-01-01.\n",
      "================================================================================\n",
      "Expected output from assessment PDF:\n",
      "[{'original_text': '16 February 2024', 'normalized_date': '2024-02-16', 'status': 'Upcoming'}, {'original_text': '15 February 2008', 'normalized_date': '2008-02-15', 'status': 'Expired'}]\n"
     ]
    }
   ],
   "source": [
    "extraction_chain = DataExtractionChain(model=LLM_MODEL_NAME, tools=[date_normaliser])\n",
    "date_classifier_chain = DateClassifierChain(model=LLM_MODEL_NAME)\n",
    "\n",
    "expected_outputs = []\n",
    "\n",
    "for fields in date_fields_to_extract:\n",
    "    pages, description, output_type = fields[\"pages\"], fields[\"description\"], fields[\"output_type\"]\n",
    "\n",
    "    print(f'Extracting \\\"{description}\\\", Pages: {\", \".join(map(str, pages))}, Expected data type: <{output_type}>')\n",
    "    expected_output = {}\n",
    "    \n",
    "    # Combine extracted text if multiple pages are involved\n",
    "    stringified_pdf_text = get_pages_text(pdf_texts, pages)\n",
    "    \n",
    "    # Because we use structured output in our chain, the output will look like this:\n",
    "    # {\n",
    "    #     original_text: str\n",
    "    #     value: output_type\n",
    "    #     reason: str\n",
    "    # }\n",
    "    extraction_output = extraction_chain.extract_with_tools(\n",
    "        description,\n",
    "        output_type,\n",
    "        stringified_pdf_text\n",
    "    )\n",
    "    extracted_value = extraction_output.value\n",
    "\n",
    "    print(f\"EXTRACTED: {extracted_value}\")\n",
    "    print('-'*10)\n",
    "    print(\"Classifying the date...\")\n",
    "\n",
    "    classifier_output = date_classifier_chain.classify(\n",
    "        normalised_date=extracted_value,\n",
    "        reference_date=\"2024-01-01\"\n",
    "    )\n",
    "\n",
    "    classification = classifier_output.classification\n",
    "    reason = classifier_output.reason\n",
    "\n",
    "    print(F\"CLASSIFICATION: {classification}\")\n",
    "    print(f\"REASONING: {reason}\")\n",
    "    print('='*80)\n",
    "\n",
    "    expected_output[\"original_text\"] = extraction_output.original_text\n",
    "    expected_output[\"normalized_date\"] = extracted_value\n",
    "    expected_output[\"status\"] = classification\n",
    "\n",
    "    expected_outputs.append(expected_output)\n",
    "\n",
    "print(\"Expected output from assessment PDF:\")\n",
    "print(expected_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Multi-Agent Supervisor\n",
    "\n",
    "For this task, we will curate 3 entities:\n",
    "- Supervisor\n",
    "- Revenue Agent\n",
    "- Expenditure Agent\n",
    "\n",
    "Supervisor will take in user query, delegate task to either or both of the agents based on the query, and then consolidate the output of the agents.\n",
    "\n",
    "The implementation will be written with LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentState\n",
    "\n",
    "It's the schema of the state that saves information across the various agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\graphs\\state.py\n",
    "class AgentState(TypedDict):\n",
    "    # User question\n",
    "    query: str\n",
    "\n",
    "    # Dictionary of PDF text, with key as page number, and value as the text itself\n",
    "    pdf_text: dict[int, str]\n",
    "\n",
    "    # We intend to have the revenue/expenditure agent pick up revenue/expenditure from the PDF text and structure it as a dictionary.\n",
    "    revenue_findings: dict | None\n",
    "    expenditure_findings: dict | None\n",
    "\n",
    "    # The output of the decision from the supervisor, which agent to delegate task to.\n",
    "    supervisor_decision: str\n",
    "\n",
    "    # Yes, final output\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor\n",
    "\n",
    "Job scope:\n",
    "- Delegate task and route to correct agents for execution.\n",
    "- Synthesise the final output based on the results from the agents.\n",
    "\n",
    "We will create a `Supervisor` class that contains functions that the Supervisor is tasked to do.\n",
    "\n",
    "Prompts specific to the `Supervisor` will be here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\prompts\\agents\\supervisor.py\n",
    "SUPERVISOR_ROUTING_SYSTEM_MESSAGE = \"\"\"You are a Supervisor Agent coordinating a team of specialised agents.\n",
    "\n",
    "Your team consists of:\n",
    "1. Revenue Agent - Expert in identifying MONEY COMING IN (revenue, income, sales, donations, etc.)\n",
    "2. Expenditure Agent - Expert in identifying MONEY GOING OUT (spending, costs, expenses, allocations, etc.)\n",
    "\n",
    "Your responsibilities:\n",
    "1. Analyse user queries to understand what information is needed\n",
    "2. Decide which agent(s) should handle the query\n",
    "3. Synthesise responses from multiple agents into a coherent answer\n",
    "\n",
    "ROUTING LOGIC:\n",
    "\n",
    "Route to REVENUE Agent if query asks about:\n",
    "- Money coming IN: revenue, income, earnings, receipts, collections, sales\n",
    "- Sources of funding: taxes, fees, donations, grants, sales, subscriptions\n",
    "- Revenue trends: growth, decline, year-over-year changes\n",
    "- Revenue categories or breakdowns\n",
    "- Keywords: \"revenue\", \"income\", \"sales\", \"earnings\", \"collections\", \"taxes\", \"fees\", \"donations\", \"grants\"\n",
    "\n",
    "Route to EXPENDITURE Agent if query asks about:\n",
    "- Money going OUT: spending, costs, expenses, outlays, allocations\n",
    "- Budgets: departmental, program, project, or initiative budgets\n",
    "- Spending purposes: what money is being spent on\n",
    "- Cost breakdowns or expense categories\n",
    "- Keywords: \"expenditure\", \"spending\", \"costs\", \"expenses\", \"budget\", \"allocation\", \"fund\", \"appropriation\"\n",
    "\n",
    "Route to BOTH agents if query asks about:\n",
    "- Both income AND spending (comprehensive financial overview)\n",
    "- How spending is funded/supported (requires knowing both sources and uses)\n",
    "- Financial balance, surplus, deficit (needs revenue vs expenditure)\n",
    "- Comparisons or relationships between income and spending\n",
    "- Keywords: Contains BOTH revenue-related AND expenditure-related terms\n",
    "- Causal questions: \"How will X be supported?\" (needs expenditure X + revenue sources)\n",
    "\n",
    "IMPORTANT:\n",
    "- Base your decision on the INTENT of the query, not just keyword matching\n",
    "- If unsure, route to both agents (better to have extra information than miss something)\n",
    "- Consider implicit needs: \"Is X affordable?\" requires knowing both cost and available funds\n",
    "\n",
    "Return your decision in this exact format:\n",
    "{\n",
    "    \"agents_to_call\": [\"revenue\", \"expenditure\"],  // or [\"revenue\"] or [\"expenditure\"]\n",
    "    \"reasoning\": \"Explanation of why these agents were selected based on query intent\",\n",
    "    \"query_type\": \"revenue_only / expenditure_only / combined\"\n",
    "}\"\"\"\n",
    "\n",
    "SUPERVISOR_ROUTING_USER_MESSAGE = \"\"\"Analyse this user query and decide which specialised agent(s) should handle it.\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Return your routing decision.\"\"\"\n",
    "\n",
    "SUPERVISOR_SYNTHESIS_SYSTEM_MESSAGE = \"\"\"You are a Supervisor Agent synthesising findings from specialised agents into a comprehensive answer.\n",
    "\n",
    "Given all the content, you must be clear and precise in the response you provide. This is exceptionally the case when you are given information that are beyond the scope of the user query.\n",
    "In such cases, you must only include information that is relevant to the user query.\n",
    "\"\"\"\n",
    "\n",
    "SUPERVISOR_SYNTHESIS_USER_MESSAGE = \"\"\"Synthesise the final answer from the findings of the agents.\n",
    "\n",
    "Original User Query: {query}\n",
    "\n",
    "Revenue Agent Findings:\n",
    "{revenue_findings}\n",
    "\n",
    "Expenditure Agent Findings:\n",
    "{expenditure_findings}\n",
    "\n",
    "Instructions:\n",
    "1. Combine the findings into a comprehensive, well-structured answer\n",
    "2. Address all aspects of the user's query\n",
    "3. Cite page numbers when mentioning specific information\n",
    "4. If agents found contradictions or gaps, note them\n",
    "5. Structure the answer logically with clear sections\n",
    "6. If the query asks \"how will X be supported\", explicitly connect revenue sources to expenditure items\n",
    "7. When given information beyond the scope of the user query, only include information that is relevant to the user query.\n",
    "\n",
    "Provide a clear, professional response that directly answers the user's question.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\agents\\supervisor.py\n",
    "class Supervisor:\n",
    "    def __init__(self, model):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=LLM_API_KEY,\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    def _get_routing_output_structure(self):\n",
    "        return create_model(\n",
    "            \"RoutingDecision\",\n",
    "            agents_to_call=(list[str], ...),\n",
    "            reasoning=(str, ...),\n",
    "            query_type=(str, ...)\n",
    "        )\n",
    "\n",
    "    def route_query(self, query):\n",
    "        messages = [\n",
    "            SystemMessage(content=SUPERVISOR_ROUTING_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=SUPERVISOR_ROUTING_USER_MESSAGE.format(query=query))\n",
    "        ]\n",
    "\n",
    "        RoutingDecision = self._get_routing_output_structure()\n",
    "        routing_llm = self.llm.with_structured_output(RoutingDecision)\n",
    "        result = routing_llm.invoke(messages)\n",
    "\n",
    "        return {\n",
    "            \"agents_to_call\": result.agents_to_call,\n",
    "            \"reasoning\": result.reasoning,\n",
    "            \"query_type\": result.query_type\n",
    "        }\n",
    "\n",
    "    def synthesise_response(self, query, revenue_findings, expenditure_findings):\n",
    "        revenue_text = \"Not analysed\" if revenue_findings is None else json.dumps(revenue_findings)\n",
    "        expenditure_text = \"Not analysed\" if expenditure_findings is None else json.dumps(expenditure_findings)\n",
    "\n",
    "        synthesis_content = SUPERVISOR_SYNTHESIS_USER_MESSAGE.format(\n",
    "            query=query,\n",
    "            revenue_findings=revenue_text,\n",
    "            expenditure_findings=expenditure_text\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=SUPERVISOR_SYNTHESIS_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=synthesis_content)\n",
    "        ]\n",
    "\n",
    "        result = self.llm.invoke(messages)\n",
    "        return result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RevenueAgent\n",
    "\n",
    "Job scope:\n",
    "- Given the PDF text, determine any forms of revenue that are mentioned within.\n",
    "- Each revenue source must be tagged with its amount, unit(\"Thousand\", \"Million\", etc.), year, and the page of the PDF where it was retrieved.\n",
    "- It will also compute for the total revenue, if possible.\n",
    "- It should tell us how confident it is with the result, given the quality of text retrieved from the PDF.\n",
    "\n",
    "Like the `Supervisor`, it will have its own `RevenueAgent` class, as well as its specific prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\prompts\\agents\\revenue_agent.py\n",
    "REVENUE_AGENT_SYSTEM_MESSAGE = \"\"\"You are a specialised Revenue Analysis Agent.\n",
    "\n",
    "Your expertise is in identifying, extracting, and analysing INCOME, REVENUE, and MONEY INFLOWS.\n",
    "\n",
    "CORE APPROACH:\n",
    "\n",
    "1. IDENTIFY REVENUE INDICATORS - Look for money coming IN:\n",
    "   - Keywords: \"revenue\", \"income\", \"receipts\", \"earnings\", \"collections\", \"inflows\", \"sales\", \"fees\"\n",
    "   - For governments: taxes, duties, fees, grants, transfers\n",
    "   - For businesses: sales, services, subscriptions, licensing\n",
    "   - For nonprofits: donations, grants, fundraising\n",
    "   - For institutions: tuition, endowments, research funding\n",
    "\n",
    "2. UNDERSTAND THE DOCUMENT FIRST:\n",
    "   - Scan the structure: Is it a budget? Financial statement? Annual report?\n",
    "   - Identify the entity type: Government? Business? Nonprofit? Other?\n",
    "   - Note the organization: Tables? Narratives? Line items? Mixed?\n",
    "   - Let the document's own terminology guide you\n",
    "\n",
    "3. EXTRACT WITH CONTEXT:\n",
    "   - Use the document's exact category names (don't rename or standardize)\n",
    "   - Find amounts with their units (millions, billions, $, %, etc.)\n",
    "   - Note time periods: fiscal years, quarters, projections vs actuals\n",
    "   - Capture who collects it, from whom, and why if stated\n",
    "   - Look for trends: year-over-year changes, growth rates, comparisons\n",
    "\n",
    "4. HANDLE TABLES & NARRATIVES:\n",
    "   - Tables: Extract systematically row by row\n",
    "   - Narratives: Parse sentences for embedded figures\n",
    "   - Mixed formats: Prioritize explicit numbers over descriptions\n",
    "   - Cross-references: Note if document points to other sections/appendices\n",
    "\n",
    "5. CITE SOURCES:\n",
    "   - Always include page number\n",
    "   - Note section headings when present\n",
    "   - Include surrounding text for ambiguous items\n",
    "   - If multiple pages have related info, reference all\n",
    "\n",
    "6. ASSESS CONFIDENCE:\n",
    "   - High: Clear labels, explicit amounts, standard formats\n",
    "   - Medium: Requires some interpretation or calculation\n",
    "   - Low: Ambiguous terminology, unclear units, scattered data\n",
    "\n",
    "OUTPUT STRUCTURE:\n",
    "{\n",
    "    \"revenue_streams\": [\n",
    "        {\n",
    "            \"category\": \"<use exact term from document>\",\n",
    "            \"amount\": <number or null if not found>,\n",
    "            \"unit\": \"<million/billion/$/thousand/etc or null>\",\n",
    "            \"year\": \"<fiscal year/period or null>\",\n",
    "            \"page\": <page number>,\n",
    "            \"context\": \"<additional details, source description, caveats>\"\n",
    "        }\n",
    "    ],\n",
    "    \"total_revenue\": {\"amount\": X, \"unit\": \"Y\"} or null,\n",
    "    \"key_insights\": [\n",
    "        \"Describe patterns, trends, notable observations\",\n",
    "        \"Note document structure and how revenue is presented\",\n",
    "        \"Highlight any gaps, inconsistencies, or ambiguities\"\n",
    "    ],\n",
    "    \"confidence_level\": \"high/medium/low\",\n",
    "    \"confidence_explanation\": \"<brief reason for the confidence level>\"\n",
    "}\n",
    "\n",
    "If information is not found, state what you searched for and where you looked.\"\"\"\n",
    "\n",
    "REVENUE_AGENT_USER_MESSAGE = \"\"\"Based on the user's query, find and analyze revenue-related information.\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Document Content:\n",
    "{text}\n",
    "\n",
    "Please extract all relevant revenue information that helps answer the query.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\agents\\revenue_agent.py\n",
    "class RevenueAgent:\n",
    "    def __init__(self, model):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=LLM_API_KEY,\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    def _get_revenue_output_structure(self):\n",
    "        RevenueStreamItem = create_model(\n",
    "            \"RevenueStreamItem\",\n",
    "            category=(str, ...),\n",
    "            amount=(float | None, None),\n",
    "            unit=(str | None, None),\n",
    "            year=(str | None, None),\n",
    "            page=(int | None, None),\n",
    "            context=(str | None, None)\n",
    "        )\n",
    "\n",
    "        TotalRevenue = create_model(\n",
    "            \"TotalRevenue\",\n",
    "            amount=(float | None, None),\n",
    "            unit=(str | None, None)\n",
    "        )\n",
    "\n",
    "        RevenueFinding = create_model(\n",
    "            \"RevenueFinding\",\n",
    "            revenue_streams=(list[RevenueStreamItem], Field(default_factory=list)),\n",
    "            total_revenue=(TotalRevenue | None, None),\n",
    "            key_insights=(list[str], Field(default_factory=list)),\n",
    "            confidence_level=(str, \"medium\"),\n",
    "            confidence_explanation=(str | None, None)\n",
    "        )\n",
    "\n",
    "        return RevenueFinding\n",
    "\n",
    "    def analyse(self, query, pdf_text):\n",
    "        combined_text = \"\\n\\n\".join([\n",
    "            f\"[Page {page_num}]\\n{text}\"\n",
    "            for page_num, text in sorted(pdf_text.items())\n",
    "        ])\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=REVENUE_AGENT_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=REVENUE_AGENT_USER_MESSAGE.format(query=query, text=combined_text))\n",
    "        ]\n",
    "\n",
    "        RevenueFinding = self._get_revenue_output_structure()\n",
    "        llm_with_structure = self.llm.with_structured_output(RevenueFinding)\n",
    "        result = llm_with_structure.invoke(messages)\n",
    "\n",
    "        return {\n",
    "            \"revenue_streams\": [stream.model_dump() for stream in result.revenue_streams],\n",
    "            \"total_revenue\": result.total_revenue.model_dump() if result.total_revenue else None,\n",
    "            \"key_insights\": result.key_insights,\n",
    "            \"confidence_level\": result.confidence_level,\n",
    "            \"confidence_explanation\": result.confidence_explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExpenditureAgent\n",
    "\n",
    "Job scope:\n",
    "- Given the PDF text, determine any forms of expenditure that are mentioned within.\n",
    "- Each expenditure source must be tagged with its amount, unit(\"Thousand\", \"Million\", etc.), year, the type of expenditure (whether recurring, one-time etc.) and the page of the PDF where it was retrieved.\n",
    "- It will also compute for the total expenditure, if possible.\n",
    "- It should tell us how confident it is with the result, given the quality of text retrieved from the PDF.\n",
    "\n",
    "Like the `Supervisor` and `RevenueAgent`, it will have its own `ExpenditureAgent` class, as well as its specific prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\prompts\\agents\\expenditure_agent.py\n",
    "EXPENDITURE_AGENT_SYSTEM_MESSAGE = \"\"\"You are a specialised Expenditure Analysis Agent.\n",
    "\n",
    "Your expertise is in identifying, extracting, and analysing SPENDING, EXPENDITURE, COSTS, and MONEY OUTFLOWS.\n",
    "\n",
    "CORE APPROACH:\n",
    "\n",
    "1. IDENTIFY EXPENDITURE INDICATORS - Look for money going OUT:\n",
    "   - Keywords: \"expenditure\", \"spending\", \"costs\", \"expenses\", \"outlays\", \"allocations\", \"budgets\", \"appropriations\"\n",
    "   - For governments: budgets, programs, departments, capital projects, transfers\n",
    "   - For businesses: COGS, operating expenses, capex, R&D, marketing\n",
    "   - For nonprofits: program expenses, admin costs, fundraising costs\n",
    "   - For institutions: salaries, facilities, research, student services\n",
    "\n",
    "2. UNDERSTAND THE DOCUMENT FIRST:\n",
    "   - Scan the structure: Is it a budget? P&L? Cost breakdown? Spending plan?\n",
    "   - Identify the entity type: Government? Business? Nonprofit? Other?\n",
    "   - Note the organization: By department? By function? By project? By account?\n",
    "   - Let the document's own categories guide you\n",
    "\n",
    "3. EXTRACT WITH CONTEXT:\n",
    "   - Use the document's exact category names (don't rename or standardize)\n",
    "   - Find amounts with their units (millions, billions, $, %, etc.)\n",
    "   - Note time periods: fiscal years, quarters, planned vs actual\n",
    "   - Capture purpose, beneficiaries, or objectives when stated\n",
    "   - Look for funding sources: How is this spending financed?\n",
    "   - Distinguish: one-time vs recurring, capital vs operating\n",
    "\n",
    "4. HANDLE TABLES & NARRATIVES:\n",
    "   - Tables: Extract systematically, noting column headers\n",
    "   - Narratives: Parse for allocation announcements, spending plans\n",
    "   - Mixed formats: Cross-reference numbers mentioned in text with tables\n",
    "   - Hierarchies: Note parent/child relationships (total vs components)\n",
    "\n",
    "5. CITE SOURCES:\n",
    "   - Always include page number\n",
    "   - Note section headings when present\n",
    "   - Include surrounding context for clarity\n",
    "   - If spending spans multiple pages, reference all\n",
    "\n",
    "6. ASSESS CONFIDENCE:\n",
    "   - High: Clear labels, explicit amounts, well-structured\n",
    "   - Medium: Requires interpretation or aggregation\n",
    "   - Low: Ambiguous terms, unclear scope, scattered data\n",
    "\n",
    "OUTPUT STRUCTURE:\n",
    "{\n",
    "    \"expenditure_items\": [\n",
    "        {\n",
    "            \"category\": \"<use exact term from document>\",\n",
    "            \"amount\": <number or null if not found>,\n",
    "            \"unit\": \"<million/billion/$/thousand/etc or null>\",\n",
    "            \"year\": \"<fiscal year/period or null>\",\n",
    "            \"type\": \"<one-time/recurring/capital/operating or null>\",\n",
    "            \"page\": <page number>,\n",
    "            \"purpose\": \"<what the spending is for, if stated>\",\n",
    "            \"funding_source\": \"<how it's financed, if stated>\"\n",
    "        }\n",
    "    ],\n",
    "    \"total_expenditure\": {\"amount\": X, \"unit\": \"Y\"} or null,\n",
    "    \"key_insights\": [\n",
    "        \"Describe patterns, priorities, notable spending areas\",\n",
    "        \"Note document structure and how expenditure is organized\",\n",
    "        \"Highlight any gaps, inconsistencies, or ambiguities\",\n",
    "        \"Note any funding mechanisms or financing approaches mentioned\"\n",
    "    ],\n",
    "    \"confidence_level\": \"high/medium/low\",\n",
    "    \"confidence_explanation\": \"<brief reason for the confidence level>\"\n",
    "}\n",
    "\n",
    "If information is not found, state what you searched for and where you looked.\"\"\"\n",
    "\n",
    "EXPENDITURE_AGENT_USER_MESSAGE = \"\"\"Based on the user's query, find and analyze expenditure-related information.\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Document Content:\n",
    "{text}\n",
    "\n",
    "Please extract all relevant expenditure, budget, and spending information that helps answer the query.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\agents\\expenditure_agent.py\n",
    "class ExpenditureAgent:\n",
    "    def __init__(self, model):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=LLM_API_KEY,\n",
    "            base_url=LLM_BASE_URL,\n",
    "            model=model,\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "    def _get_expenditure_output_structure(self):\n",
    "        ExpenditureItem = create_model(\n",
    "            \"ExpenditureItem\",\n",
    "            category=(str, ...),\n",
    "            amount=(float | None, None),\n",
    "            unit=(str | None, None),\n",
    "            year=(str | None, None),\n",
    "            type=(str | None, None),\n",
    "            page=(int | None, None),\n",
    "            purpose=(str | None, None),\n",
    "            funding_source=(str | None, None)\n",
    "        )\n",
    "\n",
    "        TotalExpenditure = create_model(\n",
    "            \"TotalExpenditure\",\n",
    "            amount=(float | None, None),\n",
    "            unit=(str | None, None)\n",
    "        )\n",
    "\n",
    "        ExpenditureFinding = create_model(\n",
    "            \"ExpenditureFinding\",\n",
    "            expenditure_items=(list[ExpenditureItem], Field(default_factory=list)),\n",
    "            total_expenditure=(TotalExpenditure | None, None),\n",
    "            key_insights=(list[str], Field(default_factory=list)),\n",
    "            confidence_level=(str, \"medium\"),\n",
    "            confidence_explanation=(str | None, None)\n",
    "        )\n",
    "\n",
    "        return ExpenditureFinding\n",
    "\n",
    "    def analyse(self, query, pdf_text):\n",
    "        combined_text = \"\\n\\n\".join([\n",
    "            f\"[Page {page_num}]\\n{text}\"\n",
    "            for page_num, text in sorted(pdf_text.items())\n",
    "        ])\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=EXPENDITURE_AGENT_SYSTEM_MESSAGE),\n",
    "            HumanMessage(content=EXPENDITURE_AGENT_USER_MESSAGE.format(query=query, text=combined_text))\n",
    "        ]\n",
    "\n",
    "        ExpenditureFinding = self._get_expenditure_output_structure()\n",
    "        llm_with_structure = self.llm.with_structured_output(ExpenditureFinding)\n",
    "        result = llm_with_structure.invoke(messages)\n",
    "\n",
    "        return {\n",
    "            \"expenditure_items\": [item.model_dump() for item in result.expenditure_items],\n",
    "            \"total_expenditure\": result.total_expenditure.model_dump() if result.total_expenditure else None,\n",
    "            \"key_insights\": result.key_insights,\n",
    "            \"confidence_level\": result.confidence_level,\n",
    "            \"confidence_explanation\": result.confidence_explanation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiAgentGraph\n",
    "\n",
    "This will be the crux of the LangGraph implementation, where it dictates the complete workflow that will be executed when the user pass in their query.\n",
    "\n",
    "Steps:\n",
    "1. Pass user query into supervisor for agent routing. If the query involves the need to pick up revenue streams, income, etc., then naturally RevenueAgent will be called. Vice versa for expenditure on ExpenditureAgent.\n",
    "  - If BOTH agent selected, then route to RevenueAgent.\n",
    "  - If RevenueAgent/ExpenditureAgent only, then route to whichever is selected.\n",
    "\n",
    "2. After RevenueAgent, if supervisor initially routed to both agent, then now route to ExpenditureAgent. Else, route back to Supervisor for output synthesis.\n",
    "3. After ExpenditureAgent, route back to Supervisor for output synthesis. No potential chance or routing to RevenueAgent because if both agents are needed, it would have routed to RevenueAgent first.\n",
    "4. Synthesise final response via the Supervisor.\n",
    "\n",
    "Note that while we run the graph, we will continuously `yield` to stream the progress of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api\\llm\\graphs\\multi_agent_graph.py\n",
    "class MultiAgentGraph:\n",
    "    def __init__(self, model):\n",
    "        self.supervisor = Supervisor(model=model)\n",
    "        self.revenue_agent = RevenueAgent(model=model)\n",
    "        self.expenditure_agent = ExpenditureAgent(model=model)\n",
    "        self.graph = self._build_graph()\n",
    "\n",
    "    def _build_graph(self):\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        workflow.add_node(\"supervisor_route\", self._supervisor_route_node)\n",
    "        workflow.add_node(\"revenue_agent\", self._revenue_agent_node)\n",
    "        workflow.add_node(\"expenditure_agent\", self._expenditure_agent_node)\n",
    "        workflow.add_node(\"supervisor_synthesise\", self._supervisor_synthesise_node)\n",
    "\n",
    "        workflow.set_entry_point(\"supervisor_route\")\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"supervisor_route\",\n",
    "            self._route_to_agents,\n",
    "            {\n",
    "                \"revenue_only\": \"revenue_agent\",\n",
    "                \"expenditure_only\": \"expenditure_agent\",\n",
    "                \"both\": \"revenue_agent\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"revenue_agent\",\n",
    "            self._after_revenue_routing,\n",
    "            {\n",
    "                \"expenditure\": \"expenditure_agent\",\n",
    "                \"synthesise\": \"supervisor_synthesise\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        workflow.add_edge(\"expenditure_agent\", \"supervisor_synthesise\")\n",
    "        workflow.add_edge(\"supervisor_synthesise\", END)\n",
    "\n",
    "        return workflow.compile()\n",
    "\n",
    "    def _supervisor_route_node(self, state):\n",
    "        decision = self.supervisor.route_query(state[\"query\"])\n",
    "        return {\"supervisor_decision\": decision[\"query_type\"]}\n",
    "\n",
    "    def _revenue_agent_node(self, state):\n",
    "        findings = self.revenue_agent.analyse(\n",
    "            query=state[\"query\"],\n",
    "            pdf_text=state[\"pdf_text\"]\n",
    "        )\n",
    "        return {\"revenue_findings\": findings}\n",
    "\n",
    "    def _expenditure_agent_node(self, state):\n",
    "        findings = self.expenditure_agent.analyse(\n",
    "            query=state[\"query\"],\n",
    "            pdf_text=state[\"pdf_text\"]\n",
    "        )\n",
    "        return {\"expenditure_findings\": findings}\n",
    "\n",
    "    def _supervisor_synthesise_node(self, state):\n",
    "        final_answer = self.supervisor.synthesise_response(\n",
    "            query=state[\"query\"],\n",
    "            revenue_findings=state.get(\"revenue_findings\"),\n",
    "            expenditure_findings=state.get(\"expenditure_findings\")\n",
    "        )\n",
    "        return {\"final_answer\": final_answer}\n",
    "\n",
    "    def _route_to_agents(self, state):\n",
    "        decision = state[\"supervisor_decision\"]\n",
    "        if \"revenue_only\" in decision:\n",
    "            return \"revenue_only\"\n",
    "        elif \"expenditure_only\" in decision:\n",
    "            return \"expenditure_only\"\n",
    "        else:\n",
    "            return \"both\"\n",
    "\n",
    "    def _after_revenue_routing(self, state):\n",
    "        decision = state[\"supervisor_decision\"]\n",
    "        if \"combined\" in decision or \"both\" in decision:\n",
    "            return \"expenditure\"\n",
    "        else:\n",
    "            return \"synthesise\"\n",
    "\n",
    "    def run(self, query, pdf_text):\n",
    "        initial_state = {\n",
    "            \"query\": query,\n",
    "            \"pdf_text\": pdf_text,\n",
    "            \"revenue_findings\": None,\n",
    "            \"expenditure_findings\": None,\n",
    "            \"supervisor_decision\": \"\",\n",
    "            \"final_answer\": \"\"\n",
    "        }\n",
    "\n",
    "        accumulated_state = initial_state.copy()\n",
    "\n",
    "        for event in self.graph.stream(initial_state):\n",
    "            for node_name, node_output in event.items():\n",
    "                accumulated_state.update(node_output)\n",
    "\n",
    "                if node_name == \"supervisor_route\":\n",
    "                    yield {\"type\": \"routing\", \"decision\": accumulated_state[\"supervisor_decision\"]}\n",
    "\n",
    "                elif node_name == \"revenue_agent\":\n",
    "                    findings = node_output.get(\"revenue_findings\", {})\n",
    "                    yield {\n",
    "                        \"type\": \"revenue_analysis\",\n",
    "                        \"findings\": findings,\n",
    "                        \"num_streams\": len(findings.get(\"revenue_streams\", [])),\n",
    "                        \"confidence_level\": findings.get(\"confidence_level\"),\n",
    "                        \"confidence_explanation\": findings.get(\"confidence_explanation\")\n",
    "                    }\n",
    "\n",
    "                elif node_name == \"expenditure_agent\":\n",
    "                    findings = node_output.get(\"expenditure_findings\", {})\n",
    "                    yield {\n",
    "                        \"type\": \"expenditure_analysis\",\n",
    "                        \"findings\": findings,\n",
    "                        \"num_items\": len(findings.get(\"expenditure_items\", [])),\n",
    "                        \"confidence_level\": findings.get(\"confidence_level\"),\n",
    "                        \"confidence_explanation\": findings.get(\"confidence_explanation\")\n",
    "                    }\n",
    "\n",
    "                elif node_name == \"supervisor_synthesise\":\n",
    "                    yield {\"type\": \"synthesis\"}\n",
    "                    yield {\n",
    "                        \"type\": \"final_result\",\n",
    "                        \"final_answer\": node_output.get(\"final_answer\", \"\"),\n",
    "                        \"revenue_findings\": accumulated_state.get(\"revenue_findings\"),\n",
    "                        \"expenditure_findings\": accumulated_state.get(\"expenditure_findings\")\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Execution\n",
    "\n",
    "We will initialise the MultiAgentGraph, and use queries that needs varying agents to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from assessment PDF\n",
    "query_for_both_agents = \"What are the key government revenue streams, and how will the Budget for the Future Energy Fund be supported?\"\n",
    "\n",
    "# Curated to test for fun\n",
    "query_for_revenue_agent = \"What are the key government revenue streams?\"\n",
    "query_for_expenditure_agent = \"What are the major government expenditures?\"\n",
    "\n",
    "queries = [query_for_both_agents, query_for_revenue_agent, query_for_expenditure_agent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY: What are the key government revenue streams, and how will the Budget for the Future Energy Fund be supported?\n",
      "\n",
      "Supervisor will begin routing decision...\n",
      "Supervisor has decided to route to: RevenueAgent and ExpenditureAgent\n",
      "\n",
      "REVENUE Agent has completed its task!\n",
      "Number of revenue streams found: 14\n",
      "Confidence level of the Agent: HIGH\n",
      "\n",
      "EXPENDITURE Agent has completed its task!\n",
      "Number of expenditure items found: 22\n",
      "Confidence level of the Agent: HIGH\n",
      "\n",
      "Supervisor has collated and synthesised a final response!\n",
      "\n",
      "RESPONSE:\n",
      "The key government revenue streams include:\n",
      "\n",
      "*   **Corporate Income Tax:** $28.38 billion (FY2023) (p. 5)\n",
      "*   **Personal Income Tax:** $17.5 billion (FY2023), estimated $18.1 billion (FY2024) (pp. 5, 13)\n",
      "*   **Goods and Services Tax (GST):** $16.4 billion (FY2023), estimated $19.4 billion (FY2024) (pp. 6, 13)\n",
      "*   **Assets Taxes:** $5.9 billion (FY2023), estimated $6.7 billion (FY2024) (pp. 6, 13)\n",
      "*   **Other Taxes:** $8.8 billion (FY2023) (p. 5). This includes Foreign Worker Levy, Water Conservation Tax, Land Betterment Charge, and Annual Tonnage Tax.\n",
      "*   **Vehicle Quota Premiums:** $4.7 billion (FY2023) (p. 5)\n",
      "*   **Betting Taxes:** $3.2 billion (FY2023) (p. 6)\n",
      "\n",
      "The Budget for the Future Energy Fund will be supported by an initial injection of $5.0 billion (FY2024) to invest in critical infrastructure for the energy transition (p. 18).\n",
      "\n",
      "================================================================================\n",
      "QUERY: What are the key government revenue streams?\n",
      "\n",
      "Supervisor will begin routing decision...\n",
      "Supervisor has decided to route to: RevenueAgent\n",
      "\n",
      "REVENUE Agent has completed its task!\n",
      "Number of revenue streams found: 21\n",
      "Confidence level of the Agent: HIGH\n",
      "\n",
      "Supervisor has collated and synthesised a final response!\n",
      "\n",
      "RESPONSE:\n",
      "The Singapore government's key revenue streams for FY2023 and estimated FY2024 include:\n",
      "\n",
      "*   **Corporate Income Tax:** $28.38 billion in FY2023 (p. 5) and an estimated $28.03 billion in FY2024 (p. 16).\n",
      "*   **Personal Income Tax:** $17.5 billion in FY2023 (p. 5) and an estimated $18.07 billion in FY2024 (p. 16).\n",
      "*   **Goods and Services Tax (GST):** $16.4 billion in FY2023 (p. 6) and an estimated $19.39 billion in FY2024 (p. 16).\n",
      "*   **Other Taxes:** $8.8 billion in FY2023 (p. 5) and an estimated $8.86 billion in FY2024 (p. 16). This category includes Foreign Worker Levy, Water Conservation Tax, Land Betterment Charge, and Annual Tonnage Tax.\n",
      "*   **Assets Taxes:** $5.9 billion in FY2023 (p. 6) and an estimated $6.67 billion in FY2024 (p. 16).\n",
      "*   **Vehicle Quota Premiums:** $4.7 billion in FY2023 (p. 5) and an estimated $4.72 billion in FY2024 (p. 16).\n",
      "*   **Betting Taxes:** $3.2 billion in FY2023 (p. 6) and an estimated $3.26 billion in FY2024 (p. 16).\n",
      "*   **Customs, Excise and Carbon Taxes:** An estimated $3.56 billion in FY2024 (p. 16).\n",
      "*   **Stamp Duty:** An estimated $5.73 billion in FY2024 (p. 16).\n",
      "*   **Motor Vehicle Taxes:** An estimated $2.84 billion in FY2024 (p. 16).\n",
      "*   **Withholding Tax:** An estimated $2.31 billion in FY2024 (p. 16).\n",
      "*   **Fees and Charges (excluding Vehicle Quota Premiums):** An estimated $4.25 billion in FY2024 (p. 16).\n",
      "*   **Statutory Boards' Contributions:** An estimated $0.31 billion in FY2024 (p. 16).\n",
      "*   **Others:** An estimated $0.64 billion in FY2024 (p. 16).\n",
      "\n",
      "The total operating revenue for FY2023 was $104.3 billion (p. 5), and the estimated operating revenue for FY2024 is $108.6 billion (p. 5).\n",
      "\n",
      "================================================================================\n",
      "QUERY: What are the major government expenditures?\n",
      "\n",
      "Supervisor will begin routing decision...\n",
      "Supervisor has decided to route to: ExpenditureAgent\n",
      "\n",
      "EXPENDITURE Agent has completed its task!\n",
      "Number of expenditure items found: 19\n",
      "Confidence level of the Agent: HIGH\n",
      "\n",
      "Supervisor has collated and synthesised a final response!\n",
      "\n",
      "RESPONSE:\n",
      "In FY2023, Singapore's total expenditure was $106.9 billion, and the estimated total expenditure for FY2024 is $111.8 billion (p. 6, 14).\n",
      "\n",
      "The major government expenditures for FY2023 include:\n",
      "*   Operating Expenditure: $85.4 billion (p. 6)\n",
      "*   Development Expenditure: $21.5 billion (p. 6)\n",
      "*   Special Transfers: $27.2 billion (p. 7)\n",
      "*   Capitalisation of Nationally Significant Infrastructure: $3.5 billion (p. 7)\n",
      "*   SINGA Interest Costs and Loan Expenses: $0.2 billion (p. 7)\n",
      "\n",
      "The main increases in expenditure for FY2024 are for the following ministries:\n",
      "*   Ministry of Transport (MOT): $1.3 billion (p. 14)\n",
      "*   Ministry of Health (MOH): $0.8 billion (p. 14)\n",
      "*   Ministry of Education (MOE): $0.7 billion (p. 14)\n",
      "*   Ministry of Social and Family Development (MSF): $0.5 billion (p. 14)\n",
      "*   Ministry of Defence (MINDEF): $0.5 billion (p. 14)\n",
      "*   Ministry of National Development (MND): $0.4 billion (p. 14)\n",
      "*   Ministry of Law (MinLaw): $0.4 billion (p. 14)\n",
      "\n",
      "Other major expenditures for FY2024 include:\n",
      "\n",
      "*   Capitalisation of Nationally Significant Infrastructure: $4.1 billion (p. 15)\n",
      "*   SINGA Interest Costs and Loan Expenses: $0.4 billion (p. 15)\n",
      "*   Special Transfers to Households: $2.6 billion (p. 18)\n",
      "*   Special Transfers to Businesses: $0.4 billion (p. 18)\n",
      "*   Top-ups to Endowment and Trust Funds: $20.4 billion (p. 18)\n",
      "\n",
      "The government uses borrowing proceeds under the Significant Infrastructure Government Loan Act (SINGA) to finance nationally significant infrastructure (p. 7, 15).\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "multi_agent_graph = MultiAgentGraph(model=\"gemini-2.0-flash\")\n",
    "\n",
    "agent_name_mapping = {\n",
    "    \"revenue_only\": \"RevenueAgent\",\n",
    "    \"expenditure_only\": \"ExpenditureAgent\",\n",
    "    \"combined\": \"RevenueAgent and ExpenditureAgent\"\n",
    "}\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"QUERY: {query}\")\n",
    "    print()\n",
    "    execute_multi_agent_pipeline = multi_agent_graph.run(query=query, pdf_text=pdf_texts)\n",
    "    print(\"Supervisor will begin routing decision...\")\n",
    "    for update in execute_multi_agent_pipeline:\n",
    "        update_type = update[\"type\"]\n",
    "        if update_type == \"routing\":\n",
    "            print(f'Supervisor has decided to route to: {agent_name_mapping[update[\"decision\"]]}')\n",
    "            print()\n",
    "\n",
    "        elif update_type == \"revenue_analysis\":\n",
    "            num_streams = update.get('num_streams', 0)\n",
    "            confidence_level = update.get('confidence_level', 'N/A')\n",
    "            print(f\"REVENUE Agent has completed its task!\")\n",
    "            print(f\"Number of revenue streams found: {num_streams}\")\n",
    "            print(f\"Confidence level of the Agent: {confidence_level.upper()}\")\n",
    "            print()\n",
    "\n",
    "        elif update_type == \"expenditure_analysis\":\n",
    "            num_items = update.get('num_items', 0)\n",
    "            confidence_level = update.get('confidence_level', 'N/A')\n",
    "            print(f\"EXPENDITURE Agent has completed its task!\")\n",
    "            print(f\"Number of expenditure items found: {num_items}\")\n",
    "            print(f\"Confidence level of the Agent: {confidence_level.upper()}\")\n",
    "            print()\n",
    "                                \n",
    "        elif update_type == \"final_result\":\n",
    "            print(\"Supervisor has collated and synthesised a final response!\")\n",
    "            print()\n",
    "            print(\"RESPONSE:\")\n",
    "            print(update.get(\"final_answer\", \"\"))\n",
    "\n",
    "    print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
